#!/usr/bin/env nix-shell
#! nix-shell --show-trace -i python3 -p python3 python35Packages.notmuch python35Packages.html2text python35Packages.chardet python35Packages.argparse

## http://machinelearning.wustl.edu/mlpapers/paper_files/icml2003_RennieSTK03.pdf
## ยง 4.4

from math import log
import sqlite3

import os
import re
import notmuch
from collections import Counter, defaultdict
import email

import html2text
import chardet

UNTRAINED = "untrained"


boring_tags = ["deleted", "inbox", "sent", "attachment", "unread", "replied",
               "flagged", "signed", "encrypted", "new", "draft",
               "meeting", "accepted", "rejected", "tentative",
               "low-importance", "normal-importance", "high-importance", UNTRAINED,
               "391"] ## 391 has very low evidence, so keeps coming up

class DB:
    def __init__(self, db_path):
        self.db = sqlite3.connect(db_path, timeout=60)
        self.db.row_factory = sqlite3.Row
        self.db.create_function("log", 1, log)
        self.dbc = self.db.cursor()
        self.create_tables()
        self.dc_cache = {}
        self.labels = set([])

    def close(self):
        self.db.commit()
        self.db.close()

    def create_tables(self):
        try:
            self.query("create table words (word text, docs int default 0, total_score real default 0, unique(word) on conflict ignore)")
            self.query("create index w on words(word)")
        except:
            pass

        try:
            self.query("insert into words (word) values ('')")
        except:
            pass

    def register_labels(self, labels):
        self.labels = set.union(self.labels, set(labels))
        for label in labels:
            try:
                self.query("alter table words add column `%s` real default 0" % (label, ))
            except:
                pass
            try:
                self.query("alter table words add column `%s_in` real default 0" % (label, ))
                self.query("alter table words add column `%s_out` real default 0"% (label, ))
            except:
                pass

    def query(self, q, x = None):
        if x == None:return self.dbc.execute(q)
        else:return self.dbc.execute(q, x)

    def document_count(self, containing_word = ''):
        if containing_word not in self.dc_cache:
            res = self.query("select docs from words where word = ?", (containing_word, )).fetchone()
            res = res[0] if res else 0
            self.dc_cache[containing_word] = res
        return self.dc_cache[containing_word]

    def idf(self, word):
        dc = self.document_count()
        dcw = self.document_count(containing_word = word)
        return log((1+dc) / (1+float(dcw)))

    def score_document(self, document, label, verbose = False):
        if verbose:
            word_scores = []

        score_in = 0
        score_out = 0
        for word in document:
            idf = self.idf(word)
            res = self.query("select `%s_in`, `%s_out` from words where word = ?" % (label, label),
                             (word, )).fetchone()
            if res:
                contrib_in  = idf * document[word] * res[0]
                contrib_out = idf * document[word] * res[1]
                score_in  = score_in  + contrib_in
                score_out = score_out + contrib_out
                if verbose:
                    word_scores.append( (score_in - score_out, word) )
        if verbose:
            word_scores.sort(reverse = True)
            print("+%-10s" % (label,), "\t", ", ".join( map(lambda x: "%12s: %.1f" % (x[1], x[0]), word_scores[:10]) ) )
            print("-%-10s" % (label,), "\t", ", ".join( map(lambda x: "%12s: %.1f" % (x[1], x[0]), word_scores[-10:]) ) )

        return (score_in, score_out)

    def update_document(self, document, labels, is_new = True):
        self.register_labels(map(lambda x : x[1], labels))
        document = {word:log(document[word] + 1) for word in document} ## TF
        norm = sum([value ** 2 for value in document.values()])**0.5
        document = {word:document[word] / norm for word in document} ## normalize

        if is_new:
            self.query("update words set docs = docs + 1 where word = ''")
        fields = ["docs = docs + 1, total_score = total_score + ?"] if is_new else []
        fields.extend(map(lambda lbl : "`%s` = max(0, `%s` + ?)" % (lbl[1], lbl[1]), labels))
        query = "update words set %s where word = ?" % (", ".join(fields), )

        for (word, score) in document.items():
            values = [score] if is_new else []
            values.extend([score * label[0] for label in labels])
            values.append(word)
            res = self.query(query, values)
            if res.rowcount == 0:
                self.query("insert into words (word) values (?)", (word, ))
                self.query(query, values)

        self.db.commit()
        self.dc_cache = {}

    def update_weights(self):
        ## compute logs
        labels = self.labels
        query = "update words set {in_fields}, {out_fields}".format(
            in_fields = ", ".join(["`{lbl}_in` = ( `{lbl}` / (select sum(`{lbl}`) from words) )".format(lbl = lbl)
                                   for lbl in labels]),
            out_fields = ", ".join(["`{lbl}_out` = ( (total_score - `{lbl}`) / (select sum(total_score - `{lbl}`) from words) )".format(lbl = lbl)
                                    for lbl in labels]))

        self.query(query)

        self.query("update words set {fields}".format(fields = ", ".join(["`{lbl}_in` = log(1 + `{lbl}_in`), `{lbl}_out` = log(1 + `{lbl}_out`)".format(lbl = lbl) for lbl in labels])))
        ## then normalize the scores - why is this good? I don't know
        fields = ", ".join([
            ("`{l}_in` = `{l}_in` / (select sum(`{l}_in`) from words), `{l}_out` = `{l}_out` / (select sum(`{l}_out`) from words)".format(l = label))
            for label in self.labels
        ])
        self.query("update words set %s" % (fields, ))
        self.db.commit()

    def reset(self):
        try:
            self.query("delete from words")
            self.query("drop table words")
            self.create_tables()
            self.register_labels(self.labels)
        except:
            pass

def scores(document, labels):
    return {label: sum([document[word] * word_weight(label, word) for word in document.keys()])
            for label in labels}

def word_weight(label, word):
    theta_hat = (count_word_not_in(label, word) + alpha_i) / (count_all_words_not_in(label) + alpha)
    log_theta_hat = log(theta_hat)

    return log_theta_hat / total_word_weight(label) # this is a nuisance

db_path = os.path.expanduser("~/.mail/classifier2.db")
db = DB(db_path)

nm = notmuch.Database(mode=notmuch.Database.MODE.READ_ONLY)
all_tags = list(nm.get_all_tags())
del nm

def not_boring(tag):
    return not(tag in boring_tags or tag.startswith('asleep-until') or tag.startswith('EXS'))

classes = set(filter(not_boring, all_tags))
db.register_labels(classes)

html_parser = html2text.HTML2Text()
html_parser.ignore_links = True
html_parser.decode_errors = 'replace'
html_parser.ignore_anchors = True
html_parser.ignore_tables = True

junk = ['body', 'head', 'pre', 'p', 'div', 'span', 'hr', 'ol', 'li',
        'com', 'org', 'net', 'uk', 'ac',
        'http', 'mailto']

bad_regex = re.compile("^(" + "|".join([ 'x[0-9a-f]{2}', '[a-f0-9]+' ] + junk) + ")$")

def is_not_junk(word):
    return len(word) > 2 and len(word) < 30 and not(bad_regex.match(word))

def good_words(txt):
    if txt == None: return []
    words = re.split("\\s|[.;:\\(\\)\\[\\]/\\!?_@-]", txt)
    words = map(lambda x : re.sub(r"[^a-z0-9']+", "", x.lower()), words)
    words = filter(is_not_junk, words)
#    print(txt, list(words))
    return words

def try_decode(payload):
    try:
        return payload.decode('utf-8')
    except:
        try:
            return payload.decode('ISO-8859-2')
        except:
            enc = chardet.detect(payload)['encoding']
            return payload.decode(enc, errors = 'replace')

def count_words(fn):
    result = Counter()

    with open(fn, errors='replace') as fin:
        msg = email.message_from_string(fin.read())
        result.update(good_words(msg['subject']))
        result.update(good_words(msg['from']))
        html_parts = []

        for part in msg.walk():
            cd = part.get("Content-Disposition", "")
            if cd.startswith("attachment"): continue
            if cd.startswith("inline") and part.get_filename(): continue
            try:
                if part.get_content_type() == 'text/plain':
                    text = try_decode(part.get_payload(decode = True))
                    result.update(good_words(text))
                elif part.get_content_type() == 'text/html':
                    html_parts.append(part)
            except:
                print('payload error in', fn)

        if sum(result.values()) < 30:
            for part in html_parts:
                html = try_decode(part.get_payload(decode = True))
                md = html_parser.handle(html)
                result.update(good_words(md))

    return result

def tagop_to_label(tag):
    if tag[0] == '+':
        return (1.5, tag[1:])
    elif tag[0] == '-':
        return (-1.5, tag[1:])
    else:
        return (1, tag)

def train(classes = classes, query=''):
    qry = '(tag:spam or not tag:spam)' +  (' and ' + query if len(query) else '')
    nm = notmuch.Database(mode=notmuch.Database.MODE.READ_WRITE)
    query = nm.create_query(qry)
    print(qry)
    count = query.count_messages()
    counter = 0
    del nm
    for msg in query.search_messages():
        fi = msg.get_filename()
        ts = list(map(tagop_to_label, set.intersection(set(msg.get_tags()), classes)))
        words = count_words(fi)
        db.update_document(words, ts)
        counter = counter + 1
        print(str(counter), '/', str(count))
        msg.remove_tag(UNTRAINED)
    db.update_weights()
    del query

def retrain(query, changed_classes):
    changed_classes = list(filter(lambda x: x[1] in classes, map(tagop_to_label, changed_classes)))

    if len(changed_classes) > 0:
        with notmuch.Database(mode=notmuch.Database.MODE.READ_ONLY) as nm:
            qry = nm.create_query(query)
            content = list([(msg.get_filename(), set(msg.get_tags()), msg.get_message_id())
                            for msg in qry.search_messages()])
            del qry
        for (fi, ts, _) in content:
            words = count_words(fi)
            if UNTRAINED in ts:
                changed_tags = set(map(lambda x:x[1], changed_classes))
                doc_changed_classes = changed_classes[:]

                for tag in ts:
                    if tag in classes and tag not in changed_tags:
                        doc_changed_classes.append( (1, tag) )

                print("learn+train", fi, doc_changed_classes)
                db.update_document(words, doc_changed_classes, is_new = False)
            else:
                print("retrain", fi, changed_classes)
                db.update_document(words, changed_classes, is_new = False)
        with notmuch.Database(mode=notmuch.Database.MODE.READ_WRITE) as nm:
            qry = nm.create_query(query) ## TODO the query could now be wrong, in principle
            for m in qry.search_messages():
                m.remove_tag(UNTRAINED)
            del qry
        ## remove all the tags on messages we changed
        db.update_weights()

def threshold(label):
    nm = notmuch.Database(mode=notmuch.Database.MODE.READ_ONLY)
    qry = nm.create_query("tag:{label}".format(label = label))
    in_threshold  = 10000000000000000000
    out_threshold = 0
    for msg in qry.search_messages():
        res = db.score_document(count_words(msg.get_filename()), label)
        t = res[0] / res[1]
        if t < in_threshold:
            in_threshold = t
            print(t, msg, msg.get_header('subject'))
    del qry
    print("".join(["-"] * 20))
    qry = nm.create_query("not tag:{label}".format(label = label))
    for msg in qry.search_messages():
        res = db.score_document(count_words(msg.get_filename()), label)
        t = res[0] / res[1]
        if t > out_threshold:
            out_threshold = t
            print(t, msg, msg.get_header('subject'))
    del nm

class_thresholds = defaultdict(lambda : float(3.4))
class_thresholds['spam'] = 7

def classify(classes = classes, query="tag:new", dry_run = False, verbose = False, one_label = None):
    nm = notmuch.Database(mode=notmuch.Database.MODE.READ_WRITE)
    new = nm.create_query(query)
    def fmt(a):
        return "% .2f %12s  %.2f %.2f" % a

    for msg in new.search_messages():
        words = count_words(msg.get_filename())
        if one_label: classes = set([one_label])
        res = {c : db.score_document(words, c, verbose = verbose) for c in classes}
        p = False
        exist = set(msg.get_tags())

        missing_but_suggested     = []
        present_but_not_suggested = []
        correctly_present         = []
        correctly_absent          = []

        for (x, (i, o)) in res.items():
            if i / o > class_thresholds[x]:
                if x not in exist: target = missing_but_suggested
                else: target = correctly_present
            elif x in exist: target = present_but_not_suggested
            else: target = correctly_absent
            target.append( (i/o, x, o, i) )
        print(msg.get_header("subject"), "  ", msg)
        for x in missing_but_suggested:
            if not(dry_run): msg.add_tag(x[1])
        missing_but_suggested.sort()
        present_but_not_suggested.sort()
        correctly_absent.sort()
        correctly_present.sort()
        for x in missing_but_suggested:
            print("+", fmt(x))
        if dry_run:
            for x in present_but_not_suggested:
                print("-", fmt(x))
            for x in correctly_present:
                print("=", fmt(x))
            for x in correctly_absent:
                print(":", fmt(x))

    del new

try:
    if __name__=="__main__":
        import sys
        import argparse

        parser = argparse.ArgumentParser()
        subs = parser.add_subparsers(dest = 'command', help='Task to perform')

        sub = subs.add_parser('threshold',
                              help="Calculate the thresholds for type 1 / type 2 errors for a tag")
        sub.add_argument("tag", type=str, choices = list(classes), help="The tag to work with")

        sub = subs.add_parser('train',
                              help = "Update the training database")
        sub.add_argument("--query", type=str, default="tag:untrained", help="What messages to train on")
        sub.add_argument("--reset", action='store_true', help='Wipe and re-train on all messages')

        sub = subs.add_parser('retrain',
                              help="Re-train on certain messages that have changed.")
        sub.add_argument("query", type=str, help="A query identifying the changed messages")
        sub.add_argument("tag", nargs='*', help="The tag changes (+tag -tag etc.)")

        sub = subs.add_parser('classify', help='Classify some messages and change their tags')
        sub.add_argument("query", type=str, help="The messages to classify", nargs='*')
        sub.add_argument("--dry-run", action='store_true', help="If specified, don't change tags")
        sub.add_argument("--verbose", action='store_true', help="If specified, print out the values each word contributes to each class")
        sub.add_argument("--tag", help="If given, only run this tag")

        args = parser.parse_args()

        if args.command == 'threshold':
            threshold(args.tag)
        elif args.command == 'train':
            if args.reset:
                db.reset()
                train()
            else:
                train(query = args.query)
        elif args.command == 'retrain':
            retrain(args.query, args.tag)
        elif args.command == 'classify':
            classify(query = ' '.join(args.query), dry_run = args.dry_run, verbose=args.verbose, one_label=args.tag)

finally:
    db.close()
